<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>important notes for speech processing </title>
</head>
<body>
    <h2> Introduction </h2>
    <p> The below link contains notes for the speech signal processing.</p>
    <a href="https://github.com/musikalkemist/AudioSignalProcessingForML/tree/master"> Lecture wise notes</a>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<h2>Important formulas</h2>
    <h3> Pitch </h3>
        <ul>
            <li> Octave is divided in 12 semitones </li>
            <li> Pitch calculation and relationship with MIDI  </li>
            <li> midi note: 69 -m A# , frequency   440 Hz </li>
            <li> F(p) = 2 <sup> (p - 69) /12</sup>. 440 </li>
            <li> F(p+1)/F(p) =   2 <sup>1/12</sup>  = 1.059 </li>
        </ul>

    <h3>Intensity, loudness and timbre </h3>
    <ul>
        <li> Threshold of hearing TOH =  10 <sup>-12</sup> W/m<sup>2</sup> </li>
        <li> Threshold of pain TOP = 10 W/m <sup>  2</sup> </li>
        <li> \(dB = 10 . log_{10} \big( \frac{I}{I_{TOH}} \big) \)</li>
        <li>Every \( \sim \) 3 dB, intensity doubles</li>
        <li> Loudness subjective perception of intensity, measured in phones</li>
        <li>Timbre: color of sound: difference between sounds with same intensity, frequency, duration: Timbre is multidimensional  </li>
        <li>Attack delay, sustain, release model (ADSR)</li>
        <li> Frequency modulation is also called as vibrato</li>
        <li> Amplitude modulation - tremelo</li>
    </ul>

    <h3>Audio signals</h3>
    <h4> Analog to Ditital conversion </h4>
        <ul>
        <li> Sampling \(  t_n = n. T  \) </li>
        <li>Quantization</li>
        <li> Sampling rate  \( s_r = \frac{1}{T} \) </li>
        <li> Why sampling rate for CD 44100 hz ? </li>
        <li> Nyquist sampling \( f_N = s_r / 2\) </li>
        <li> nyquist frequency for CD 44100 /2 = 22050 - we can go upto 22k freq with no artifacts. </li>
        <li> Aliasing </li>
        <li> Quantization: values along y-axis </li>
        <li>resolution: number of bits, bit depth: (same), for example for CD: 16 bit depth, \( 2^{16} =  655356\) </li>
        <li> Memory requirement for 1 minute of audio: bit depth: 16, sampling rate 44100 Hz</li>
        <li> (16 x 44100 /1048576)/8 x 60 = 5.49 MB </li>
        <li> Dynamic range: Difference between largest/smallest signal a system can record</li> 
        <li> Signal to quantization noise ratio: relationship btween max signal strengh and quantization error </li>
        <li> SQNR = 6.02 Q (check)  </li>
    </ul>

    <h4> How to record sound (ADC), how to reproudce sound (DAC) </h4> 

<h3> Audio Features </h3>
    <h4> Level of abstraction </h4>
    <ul>
        <li> High-level: 
                Examples: instrumentation, key, chords, melody, rhythm,
                tempo, lyrics, genre, mood </li>
        <li>  Mid-level:
                Examples: pitch- and heat-related descriptors, such as note
                onsets, fluctuation patterns, MFCCs </li>
        <li> Low-level: 
                level of abstraction, temporal scope , music aspect, signal domain, ML approach </li>
    </ul>

    <h4>Temporal scope</h4>
        <li>Applies to music as well as non music</li>
        <li> Instantaneous (~50 ms), Segment level (seconds), Global</li>

    <h4>Signal domain</h4>
    <ul>
        <li> Time domain</li>
        <li>Frequency domain: Band energy ration, </li> 
        <li> Time frequency representation: Spectrogram, Mel-spectrogram, constant q-transform</li>
        <ul>
            <li>Spectrogram: short term fourier transform</li>
        </ul>
            <li> Traditional machine learning </li>
            <ul>
                <li> Amplitude envelope, Root mean square energy, zero crossing rate, band energy ratio, spectral centriod, 
                spectral flux, spectral spread, spectral roll-off </li> 
            </ul>
    </ul>

<h3> Time domain feature pipeline </h3>
<ul>
    <li> Audio , ADC,sampling and quatization, framing (overlapping) </li>
    <li>Reason for frame overlapping: 1 sample: 1@44.1 KHz = 0.0227 ms, ear's resolution: 10 ms</li>
    <li>  frame size 128 - power of 2, if number of samples are power of 2, fast fourier transform is faster </li>
    <li> Typical values: 256 - 8192</li>
    <li> \( d_f = \frac{1}{s_r}.K  = \frac{1}{44100}. 512 = 11.6 ms \) </li>
    <li> after framing: feature computation, aggregation (mean, median, GMM) , feature vector/vector/matrix
</ul>

<h3> Frequency domain feature pipeline </h3>
<h3> dfdf </h3>
    <ul>
        <li> Audio , ADC,sampling and quatization, </li>
        <li> Spectral leakage </li>
        <ul>
            <li> Processed signal is not an integer number of period </li>
            <li> End points are discontinous </li>
            <li> Discontinuties appear as high frequency components not present in the signal </li>
            <li> How to get over with it : Windowing  </li:>
                <ul>
                    <li>Apply windowing function to each frame</li>
                    <li> Eliminating samples from both ends of a frame </li>
                    <li> Hann Window: \( w(k) = 0.5 \bigg( 1 - \cos \big(\frac{2 \pi k }{K -1 } \big) \bigg)  ; k = 1....k \) </li>
                    <li> \( s_k (k) = s(k) . w(k) , k = 1 ... k\) </li>
                    <li> So this makes signal zero near ends and we loose the signal </li>
                </ul>
        </ul>
    </ul>
<h3>Time domain audio features </h3>
    <ul>
        <li>Amplitude envelope (AE)</li>
        <ul>
            <li> max amplitude value of all samples in a frame </li>
            <li> \( AE_t = \max_{k = t.K}^{(t+1).K -1 }s(k) \) </li>
            <li> Calculate for all the envelopes/frames</li> 
            <li> It gives rough idea of loudness, but sensitive to outliers.</li>
            <li> Use case: onset detection, music genre classification.</li>
        </ul>
        <li>Root mean square energy (RMS)</li>
        <ul>
            <li>RMS of all samples of a frame. </li>
            <li> \( RMS_t = \sqrt {\frac{1}{K} \sum_{k = t.K}^{(t+1)K -1} s(k)^2} \) </li>
            <li> Indicator of loudness, less sensitive to outliers. </li>
            <li> applications: audio segmentation, music genre classification.</li>
        <li> Zero crossing rate (ZCR)  </li>
        <ul>
            <li>number of times signal crosses the horizontal axis</li>
            <li> \( ZCR_t = \frac{1}{2} \sum_{k = t.K}^{(t+1).K -1}  |sgn (s(k)) - sgn (s(k +1)) | \)</li>
        </ul>

    </ul>

</body>
</html>