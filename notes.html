<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>important notes for speech processing </title>
</head>
<body>
    <h2> Introduction </h2>
    <p> The below link contains notes for the speech signal processing.</p>
    <a href="https://github.com/musikalkemist/AudioSignalProcessingForML/tree/master"> Lecture wise notes</a>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<h2>Important formulas</h2>
    <h3> Pitch </h3>
        <ul>
            <li> Octave is divided in 12 semitones </li>
            <li> Pitch calculation and relationship with MIDI  </li>
            <li> midi note: 69 -m A# , frequency   440 Hz </li>
            <li> F(p) = 2 <sup> (p - 69) /12</sup>. 440 </li>
            <li> F(p+1)/F(p) =   2 <sup>1/12</sup>  = 1.059 </li>
        </ul>

    <h3>Intensity, loudness and timbre </h3>
    <ul>
        <li> Threshold of hearing TOH =  10 <sup>-12</sup> W/m<sup>2</sup> </li>
        <li> Threshold of pain TOP = 10 W/m <sup>  2</sup> </li>
        <li> \(dB = 10 . log_{10} \big( \frac{I}{I_{TOH}} \big) \)</li>
        <li>Every \( \sim \) 3 dB, intensity doubles</li>
        <li> Loudness subjective perception of intensity, measured in phones</li>
        <li>Timbre: color of sound: difference between sounds with same intensity, frequency, duration: Timbre is multidimensional  </li>
        <li>Attack delay, sustain, release model (ADSR)</li>
        <li> Frequency modulation is also called as vibrato</li>
        <li> Amplitude modulation - tremelo</li>
    </ul>

    <h3>Audio signals</h3>
    <h4> Analog to Ditital conversion </h4>
        <ul>
        <li> Sampling \(  t_n = n. T  \) </li>
        <li>Quantization</li>
        <li> Sampling rate  \( s_r = \frac{1}{T} \) </li>
        <li> Why sampling rate for CD 44100 hz ? </li>
        <li> Nyquist sampling \( f_N = s_r / 2\) </li>
        <li> nyquist frequency for CD 44100 /2 = 22050 - we can go upto 22k freq with no artifacts. </li>
        <li> Aliasing </li>
        <li> Quantization: values along y-axis </li>
        <li>resolution: number of bits, bit depth: (same), for example for CD: 16 bit depth, \( 2^{16} =  655356\) </li>
        <li> Memory requirement for 1 minute of audio: bit depth: 16, sampling rate 44100 Hz</li>
        <li> (16 x 44100 /1048576)/8 x 60 = 5.49 MB </li>
        <li> Dynamic range: Difference between largest/smallest signal a system can record</li> 
        <li> Signal to quantization noise ratio: relationship btween max signal strengh and quantization error </li>
        <li> SQNR = 6.02 Q (check)  </li>
    </ul>

    <h4> How to record sound (ADC), how to reproudce sound (DAC) </h4> 

<h3> Audio Features </h3>
    <h4> Level of abstraction </h4>
    <ul>
        <li> High-level: 
                Examples: instrumentation, key, chords, melody, rhythm,
                tempo, lyrics, genre, mood </li>
        <li>  Mid-level:
                Examples: pitch- and heat-related descriptors, such as note
                onsets, fluctuation patterns, MFCCs </li>
        <li> Low-level: 
                level of abstraction, temporal scope , music aspect, signal domain, ML approach </li>
    </ul>

    <h4>Temporal scope</h4>
        <li>Applies to music as well as non music</li>
        <li> Instantaneous (~50 ms), Segment level (seconds), Global</li>

    <h4>Signal domain</h4>
    <ul>
        <li> Time domain</li>
        <li>Frequency domain: Band energy ration, </li> 
        <li> Time frequency representation: Spectrogram, Mel-spectrogram, constant q-transform</li>
        <ul>
            <li>Spectrogram: short term fourier transform</li>
        </ul>
            <li> Traditional machine learning </li>
            <ul>
                <li> Amplitude envelope, Root mean square energy, zero crossing rate, band energy ratio, spectral centriod, 
                spectral flux, spectral spread, spectral roll-off </li> 
            </ul>
    </ul>

<h3> Time domain feature pipeline </h3>
<ul>
    <li> Audio , ADC,sampling and quatization, framing (overlapping) </li>
    <li>Reason for frame overlapping: 1 sample: 1@44.1 KHz = 0.0227 ms, ear's resolution: 10 ms</li>
    <li>  frame size 128 - power of 2, if number of samples are power of 2, fast fourier transform is faster </li>
    <li> Typical values: 256 - 8192</li>
    <li> \( d_f = \frac{1}{s_r}.K  = \frac{1}{44100}. 512 = 11.6 ms \) </li>
    <li> after framing: feature computation, aggregation (mean, median, GMM) , feature vector/vector/matrix
</ul>

<h3> Frequency domain feature pipeline </h3>
    <ul>
        <li> Audio , ADC,sampling and quatization, </li>
        <li> Spectral leakage </li>
        <ul>
            <li> Processed signal is not an integer number of period </li>
            <li> End points are discontinous </li>
            <li> Discontinuties appear as high frequency components not present in the signal </li>
            <li> How to get over with it : Windowing  </li:>
                <ul>
                    <li>Apply windowing function to each frame</li>
                    <li> Eliminating samples from both ends of a frame </li>
                    <li> Hann Window: \( w(k) = 0.5 \bigg( 1 - \cos \big(\frac{2 \pi k }{K -1 } \big) \bigg)  ; k = 1....k \) </li>
                    <li> \( s_k (k) = s(k) . w(k) , k = 1 ... k\) </li>
                    <li> So this makes signal zero near ends and we loose the signal </li>
                </ul>
        </ul>
    </ul>
<h3>Time domain audio features </h3>
    <ul>
        <li>Amplitude envelope (AE)</li>
        <ul>
            <li> max amplitude value of all samples in a frame </li>
            <li> \( AE_t = \max_{k = t.K}^{(t+1).K -1 }s(k) \) </li>
            <li> Calculate for all the envelopes/frames</li> 
            <li> It gives rough idea of loudness, but sensitive to outliers.</li>
            <li> Use case: onset detection, music genre classification.</li>
        </ul>
        <li>Root mean square energy (RMS)</li>
        <ul>
            <li>RMS of all samples of a frame. </li>
            <li> \( RMS_t = \sqrt {\frac{1}{K} \sum_{k = t.K}^{(t+1)K -1} s(k)^2} \) </li>
            <li> Indicator of loudness, less sensitive to outliers. </li>
            <li> applications: audio segmentation, music genre classification.</li>
        </ul>
        <li> Zero crossing rate (ZCR)  </li>
        <ul>
            <li>number of times signal crosses the horizontal axis</li>
            <li> \( ZCR_t = \frac{1}{2} \sum_{k = t.K}^{(t+1).K -1}  |sgn (s(k)) - sgn (s(k +1)) | \)</li>
            <li> recognition of percussive versus pitched sounds, monophonic pitch estimation (check)</li>
        </ul>

    </ul>

<h3>Fourier Transform</h3>
<h4>Introduction</h4>
<ul>
    <li>The prism example: white light splitting into different colors</li>
    <li>Decompose a complex sound into its frequency components</li>
    <li>From time domain to frequency domain</li>
    <li>Compare signal with sinusoids of various frequency: each freq: magnitude and phase</li>
    <li> High magnitude indicates higher similarity</li>
    <li>C note example: good example. freq along with higher octaves.</li>
    <li> Fourier transform steps:</li>
    <ul>
        <li>Choose a frequency</li>
        <li> Optimize phase</li>
        <li>Calculate magnitude</li>
        <li>  \(  \phi_f = argmax_{\phi \in [0,1) }  \bigg(  \int s(t) . sin (2\pi (f.t - \phi)) \bigg) \)</li>
        <li>  \(  d_f = max_{\phi \in [0,1) }  \bigg(  \int s(t) . sin (2\pi (f.t - \phi)) \bigg) \) </li>
        <li>Assumption: time and frequency are continous.</li>
    </ul>
    <li>How to go in reverse: frequency domain to time domain.</li>
    <ul>
        <li>Superimpose sinusiods.</li>
        <li> Weigh them by their relative magnitude.</li>
        <li> Use relative phase.</li>
        <li>Additive synthesis (synthesizers): for creating complex music sounds.  <a href="https://teropa.info/harmonics-explorer/">Teropa</a></li>
    </ul>
    <li> Complex number why in fourier domain?</li>
    <ul>
        <li> Fourier transform: magnitude and phase : something with magnitude + phase</li>
        <li> \( c = a + i.b ,  \gamma = \tan^{-1} (b/a),  |c| = \sqrt{(a^2 + b^2)} \)</li>
        <li> c =  \(|c| \big(cos (\gamma) + i sin(\gamma) \big)\)</li>
        <li>\( e^{i\gamma} = cos(\gamma) + i sin(\gamma) \)</li>
        <li>euler's identity: \( e^{i \pi} + 1 = 0 \) </li>
        <li>\( c = |c| e^{i\gamma}\) </li>
    </ul>
    <li> Fourier transform using complex numbers</li>
    <ul>
        <li>Using previous two formula for fourier transform phase and magnitude and complex number definition</li>
        <li>\( c_f = \frac{d_f}{\sqrt{2}} e^{-i2\pi \phi_f}\)</li>
        <li> Here due to - sign, rotating clockwise direction.</li>
        <li>\(  \hat s (f)  = c_f\)</li>
        <li> \( \hat s (f)  = \int s(t). e^{-i 2\pi ft} dt\) </li>
        <li> At every point t, we are rotating full circle with speed dependent upon t, if t is more, speed is less.</li>
        <li>  \( s(t). e^{-2 \pi f t} \)- this is stable when that frequency is present otherwise not</li>
        <li> integral reprsenets the center of gravity: it is zero if curve is more unstable in general - represents the   </li>
        
    </ul>
    <li> Inverse Fourier transform:</li>
    <ul>
        <li> \( g(t) = \int c_f  . e^{2\pi i ft} df\) </li>
    </ul>
</ul>
<h3>Discrete Fourier Transform</h3>
<ul>
    <li>\( \hat{g}(f) =    \int g(t). e^{-i 2\pi f t} dt \)</li>
    <li> The above formula works with analog signals</li>
    <li>Digital signal: \( g(t)   \mapsto  x(n) \)</li>
    <li> \(t = nT \)</li>
    <li> Building a discrete fourier transform</li>
    <li> \( \hat{x(f)} =    \sum_n x(n) e^{-2 i \pi f n  }\) </li>
    <li>Above definition has issues with continous frequency and infinite time.</li>
    <li>Hacks: Consider f to be non 0 in a finite time interval.</li>
    <lI> compute transform for finite number of frequencies.</lI>
    <li>Number of frequencies (M) = Number of samples (N)</li>
    <li> Why M = N?</li>
    <ul>
        <li> Invertible transformation</li> 
        <li> Computational efficiency</li>
    </ul>
    <li>\( \hat{x}(f) =  \sum_{n=0}^{N-1} x(n) . e^{-i 2 \pi f n}\) </li>
    <li>\( \hat{x}(k/N) =  \sum_{n=0}^{N-1} x(n) . e^{-i 2 \pi n k/N }\) </li>
    <li> k = [0, M-1] = [0 - N-1] </li>
    <li>\( F(k) = \frac{k}{NT}  = \frac{k s_r}{N}\)</li>
    <li> Redundancy in DFT</li>
    <li>\( k = N/2 \rightarrow F(N/2) = s_r/2 \)  \(\rightarrow\) Nyquist frequency</li>
    <li>So above nyquist frequency, we are not able to reconstruct the signal without some aliasing.</li>
</ul>
<h3>From DFT to Fast Fourier Transform</h3>
<ul>
    <li>DFT is computationaly expensive \(  O(N^2)\) </li>
    <li>FFT is more efficient \( O(N \log_2(N) ) \)</li>
    <li> FFT exploits redundancies across sinusoids</li>
    <li> FFT works when N is a power of 2. </li>
</ul>
</body>
</html>