<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
    <title>important notes for speech processing </title>
</head>
<style>
    
    #myChartmel { border: 1px solid gray; }
    
  </style>
<body>
    <h2> Introduction </h2>
    <p> The below link contains notes for the speech signal processing.</p>
    <a href="https://github.com/musikalkemist/AudioSignalProcessingForML/tree/master"> Lecture wise notes</a>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<h2>Important formulas</h2>
    <h3> Pitch </h3>
        <ul>
            <li> Octave is divided in 12 semitones </li>
            <li> Pitch calculation and relationship with MIDI  </li>
            <li> midi note: 69 -m A# , frequency   440 Hz </li>
            <li> F(p) = 2 <sup> (p - 69) /12</sup>. 440 </li>
            <li> F(p+1)/F(p) =   2 <sup>1/12</sup>  = 1.059 </li>
        </ul>

    <h3>Intensity, loudness and timbre </h3>
    <ul>
        <li> Threshold of hearing TOH =  10 <sup>-12</sup> W/m<sup>2</sup> </li>
        <li> Threshold of pain TOP = 10 W/m <sup>  2</sup> </li>
        <li> \(dB = 10 . log_{10} \big( \frac{I}{I_{TOH}} \big) \)</li>
        <li>Every \( \sim \) 3 dB, intensity doubles</li>
        <li> Loudness subjective perception of intensity, measured in phones</li>
        <li>Timbre: color of sound: difference between sounds with same intensity, frequency, duration: Timbre is multidimensional  </li>
        <li>Attack delay, sustain, release model (ADSR)</li>
        <li> Frequency modulation is also called as vibrato</li>
        <li> Amplitude modulation - tremelo</li>
    </ul>

    <h3>Audio signals</h3>
    <h4> Analog to Ditital conversion </h4>
        <ul>
        <li> Sampling \(  t_n = n. T  \) </li>
        <li>Quantization</li>
        <li> Sampling rate  \( s_r = \frac{1}{T} \) </li>
        <li> Why sampling rate for CD 44100 hz ? </li>
        <li> Nyquist sampling \( f_N = s_r / 2\) </li>
        <li> nyquist frequency for CD 44100 /2 = 22050 - we can go upto 22k freq with no artifacts. </li>
        <li> Aliasing </li>
        <li> Quantization: values along y-axis </li>
        <li>resolution: number of bits, bit depth: (same), for example for CD: 16 bit depth, \( 2^{16} =  655356\) </li>
        <li> Memory requirement for 1 minute of audio: bit depth: 16, sampling rate 44100 Hz</li>
        <li> (16 x 44100 /1048576)/8 x 60 = 5.49 MB </li>
        <li> Dynamic range: Difference between largest/smallest signal a system can record</li> 
        <li> Signal to quantization noise ratio: relationship btween max signal strengh and quantization error </li>
        <li> SQNR = 6.02 Q (check)  </li>
    </ul>

    <h4> How to record sound (ADC), how to reproudce sound (DAC) </h4> 

<h3> Audio Features </h3>
    <h4> Level of abstraction </h4>
    <ul>
        <li> High-level: 
                Examples: instrumentation, key, chords, melody, rhythm,
                tempo, lyrics, genre, mood </li>
        <li>  Mid-level:
                Examples: pitch- and heat-related descriptors, such as note
                onsets, fluctuation patterns, MFCCs </li>
        <li> Low-level: 
                level of abstraction, temporal scope , music aspect, signal domain, ML approach </li>
    </ul>

    <h4>Temporal scope</h4>
        <li>Applies to music as well as non music</li>
        <li> Instantaneous (~50 ms), Segment level (seconds), Global</li>

    <h4>Signal domain</h4>
    <ul>
        <li> Time domain</li>
        <li>Frequency domain: Band energy ration, </li> 
        <li> Time frequency representation: Spectrogram, Mel-spectrogram, constant q-transform</li>
        <ul>
            <li>Spectrogram: short term fourier transform</li>
        </ul>
            <li> Traditional machine learning </li>
            <ul>
                <li> Amplitude envelope, Root mean square energy, zero crossing rate, band energy ratio, spectral centriod, 
                spectral flux, spectral spread, spectral roll-off </li> 
            </ul>
    </ul>

<h3> Time domain feature pipeline </h3>
<ul>
    <li> Audio , ADC,sampling and quatization, framing (overlapping) </li>
    <li>Reason for frame overlapping: 1 sample: 1@44.1 KHz = 0.0227 ms, ear's resolution: 10 ms</li>
    <li>  frame size 128 - power of 2, if number of samples are power of 2, fast fourier transform is faster </li>
    <li> Typical values: 256 - 8192</li>
    <li> \( d_f = \frac{1}{s_r}.K  = \frac{1}{44100}. 512 = 11.6 ms \) </li>
    <li> after framing: feature computation, aggregation (mean, median, GMM) , feature vector/vector/matrix
</ul>

<h3> Frequency domain feature pipeline </h3>
    <ul>
        <li> Audio , ADC,sampling and quatization, </li>
        <li> Spectral leakage </li>
        <ul>
            <li> Processed signal is not an integer number of period </li>
            <li> End points are discontinous </li>
            <li> Discontinuties appear as high frequency components not present in the signal </li>
            <li> How to get over with it : Windowing  </li:>
                <ul>
                    <li>Apply windowing function to each frame</li>
                    <li> Eliminating samples from both ends of a frame </li>
                    <li> Hann Window: \( w(k) = 0.5 \bigg( 1 - \cos \big(\frac{2 \pi k }{K -1 } \big) \bigg)  ; k = 1....k \) </li>
                    <li> \( s_k (k) = s(k) . w(k) , k = 1 ... k\) </li>
                    <li> So this makes signal zero near ends and we loose the signal </li>
                </ul>
        </ul>
    </ul>
<h3>Time domain audio features </h3>
    <ul>
        <li>Amplitude envelope (AE)</li>
        <ul>
            <li> max amplitude value of all samples in a frame </li>
            <li> \( AE_t = \max_{k = t.K}^{(t+1).K -1 }s(k) \) </li>
            <li> Calculate for all the envelopes/frames</li> 
            <li> It gives rough idea of loudness, but sensitive to outliers.</li>
            <li> Use case: onset detection, music genre classification.</li>
        </ul>
        <li>Root mean square energy (RMS)</li>
        <ul>
            <li>RMS of all samples of a frame. </li>
            <li> \( RMS_t = \sqrt {\frac{1}{K} \sum_{k = t.K}^{(t+1)K -1} s(k)^2} \) </li>
            <li> Indicator of loudness, less sensitive to outliers. </li>
            <li> applications: audio segmentation, music genre classification.</li>
        </ul>
        <li> Zero crossing rate (ZCR)  </li>
        <ul>
            <li>number of times signal crosses the horizontal axis</li>
            <li> \( ZCR_t = \frac{1}{2} \sum_{k = t.K}^{(t+1).K -1}  |sgn (s(k)) - sgn (s(k +1)) | \)</li>
            <li> recognition of percussive versus pitched sounds, monophonic pitch estimation (check)</li>
        </ul>

    </ul>

<h3>Fourier Transform</h3>
<h4>Introduction</h4>
<ul>
    <li>The prism example: white light splitting into different colors</li>
    <li>Decompose a complex sound into its frequency components</li>
    <li>From time domain to frequency domain</li>
    <li>Compare signal with sinusoids of various frequency: each freq: magnitude and phase</li>
    <li> High magnitude indicates higher similarity</li>
    <li>C note example: good example. freq along with higher octaves.</li>
    <li> Fourier transform steps:</li>
    <ul>
        <li>Choose a frequency</li>
        <li> Optimize phase</li>
        <li>Calculate magnitude</li>
        <li>  \(  \phi_f = argmax_{\phi \in [0,1) }  \bigg(  \int s(t) . sin (2\pi (f.t - \phi)) \bigg) \)</li>
        <li>  \(  d_f = max_{\phi \in [0,1) }  \bigg(  \int s(t) . sin (2\pi (f.t - \phi)) \bigg) \) </li>
        <li>Assumption: time and frequency are continous.</li>
    </ul>
    <li>How to go in reverse: frequency domain to time domain.</li>
    <ul>
        <li>Superimpose sinusiods.</li>
        <li> Weigh them by their relative magnitude.</li>
        <li> Use relative phase.</li>
        <li>Additive synthesis (synthesizers): for creating complex music sounds.  <a href="https://teropa.info/harmonics-explorer/">Teropa</a></li>
    </ul>
    <li> Complex number why in fourier domain?</li>
    <ul>
        <li> Fourier transform: magnitude and phase : something with magnitude + phase</li>
        <li> \( c = a + i.b ,  \gamma = \tan^{-1} (b/a),  |c| = \sqrt{(a^2 + b^2)} \)</li>
        <li> c =  \(|c| \big(cos (\gamma) + i sin(\gamma) \big)\)</li>
        <li>\( e^{i\gamma} = cos(\gamma) + i sin(\gamma) \)</li>
        <li>euler's identity: \( e^{i \pi} + 1 = 0 \) </li>
        <li>\( c = |c| e^{i\gamma}\) </li>
    </ul>
    <li> Fourier transform using complex numbers</li>
    <ul>
        <li>Using previous two formula for fourier transform phase and magnitude and complex number definition</li>
        <li>\( c_f = \frac{d_f}{\sqrt{2}} e^{-i2\pi \phi_f}\)</li>
        <li> Here due to - sign, rotating clockwise direction.</li>
        <li>\(  \hat s (f)  = c_f\)</li>
        <li> \( \hat s (f)  = \int s(t). e^{-i 2\pi ft} dt\) </li>
        <li> At every point t, we are rotating full circle with speed dependent upon t, if t is more, speed is less.</li>
        <li>  \( s(t). e^{-2 \pi f t} \)- this is stable when that frequency is present otherwise not</li>
        <li> integral reprsenets the center of gravity: it is zero if curve is more unstable in general - represents the   </li>
        
    </ul>
    <li> Inverse Fourier transform:</li>
    <ul>
        <li> \( g(t) = \int c_f  . e^{2\pi i ft} df\) </li>
    </ul>
</ul>
<h3>Discrete Fourier Transform</h3>
<ul>
    <li>\( \hat{g}(f) =    \int g(t). e^{-i 2\pi f t} dt \)</li>
    <li> The above formula works with analog signals</li>
    <li>Digital signal: \( g(t)   \mapsto  x(n) \)</li>
    <li> \(t = nT \)</li>
    <li> Building a discrete fourier transform</li>
    <li> \( \hat{x(f)} =    \sum_n x(n) e^{-2 i \pi f n  }\) </li>
    <li>Above definition has issues with continous frequency and infinite time.</li>
    <li>Hacks: Consider f to be non 0 in a finite time interval.</li>
    <lI> compute transform for finite number of frequencies.</lI>
    <li>Number of frequencies (M) = Number of samples (N)</li>
    <li> Why M = N?</li>
    <ul>
        <li> Invertible transformation</li> 
        <li> Computational efficiency</li>
    </ul>
    <li>\( \hat{x}(f) =  \sum_{n=0}^{N-1} x(n) . e^{-i 2 \pi f n}\) </li>
    <li>\( \hat{x}(k/N) =  \sum_{n=0}^{N-1} x(n) . e^{-i 2 \pi n k/N }\) </li>
    <li> k = [0, M-1] = [0 - N-1] </li>
    <li>\( F(k) = \frac{k}{NT}  = \frac{k s_r}{N}\)</li>
    <li> Redundancy in DFT</li>
    <li>\( k = N/2 \rightarrow F(N/2) = s_r/2 \)  \(\rightarrow\) Nyquist frequency</li>
    <li>So above nyquist frequency, we are not able to reconstruct the signal without some aliasing.</li>
</ul>
<h3>From DFT to Fast Fourier Transform</h3>
<ul>
    <li>DFT is computationaly expensive \(  O(N^2)\) </li>
    <li>FFT is more efficient \( O(N \log_2(N) ) \)</li>
    <li> FFT exploits redundancies across sinusoids</li>
    <li> FFT works when N is a power of 2. </li>
</ul>
<h3>Short-Time fourier Transform</h3>
<ul>
    <li>Initial fft only represents the frequency components present in entire signal but not sure where.</li>
    <li>Consider small segments of the signal, apply FFT logically</li>
    <li>Apply windowing function to the signal</li>
    <li>\( x_w (k) = x(k).w(k) \)</li>
    <li>example:rectangle window function rect some place and 0 other places</li>
    <li> window size and frame size measured in number of samples</li>
    <li>usually window size and frame size coinside, but sometimes frame size is larger</li>
    <li>window function + remaining 0 padded</li>
    <li> for now assume window size == frame size</li>
    <li>But frames are overlapping (Hop size: how many samples to slide for taking the next frame)</li>
    <li>\( \hat{x}(k) =  \sum_{n=0}^{N-1} x(n) . e^{-i 2 \pi n k/N }\) </li>
    <li>\( \hat{S}(m,k) =  \sum_{n=0}^{N-1} x(n + mH). w(n) . e^{-i 2 \pi n k/N }\) </li>
    <li> DFT</li>
    <ul>
        <li>Spectral vector (# of frequency bins)</li>
        <li>N Complex fourier cofficients</li>
    </ul>
    <li>STFT</li>
    <ul>
        <li>Spectral matrix (# frequency bins, #frames)</li>
        <li>Complex fourier cofficients</li>
    </ul>
    <li># frequency bins = \(\frac{framesize}{2} + 1\) </li>
    <li># frames = \( \frac{samples - framesize}{hopsize} + 1\)</li>
    <li>Time / frequency tradeoff</li>
    <ul>
        <li>if  frame size \( \Uparrow \), freq resolution \( \Uparrow \)  and time resolution \( \Downarrow\) (Reason is more frequency bins)</li>
        <li>if  frame size \( \Downarrow \), freq resolution \( \Downarrow \)  and time resolution \( \Uparrow\)</li>
        <li>Windowing function: Hann window \(w(k) =  0.5 . (1 - cos(\frac{2 \pi k}{K-1})) ; k = (0,....K) \)</li>
        <li> If we simply visualize the spectrogram, it does not plot values which we can see visually since values are very small.</li>
        <li> Therefore we feed the log of the amplitude (librosa power_to_db function: computes on square of amplitde given as input) </li>
        <li>The above thing solves the purpose, however there is one more catch as we do not perceive the frequencies linearly, that also should be logarithmic</li>
        <li> Therefore the steps can be summarized as follows:</li>
        <ul>
            <li>Given signal, compute the STFT</li>
            <li>compute the absolute value of the ouput</li>
            <li>Take squared value of the absolute output, compute the db</li>
            <li>plot it on the log scale frequency</li>
        </ul>
    </ul>
</ul>
    <h3>Mel-spectrograms</h3>
    <ul>
        <li>Psychoacoustic exeperiment</li>
        <ul>
            <li>1st sample: C2-C4 (65 - 262 Hz) </li>
            <li>2nd sample: G6-A6 (1568-1760 Hz) </li>
            <li>Both have around 200 Hz difference, but when we hear, we percieve/feel that the first one has more difference.</li>
            <li>We have better resolution at low frequency than the higher frequency.</li>
            <li>Humans perceive frequency logarithmically.</li>
        </ul>
        <li> Ideal audio feature:</li>
        <ul>
            <li>Iime frequency representation (simple spectrogram can do)</li>
            <li>Perceptually-relevant amplitude representation (simple spectrogram can do)</li>
            <li>Perceptually-relevant frequency representation (cannot do)</li>
        </ul>
        <li>Mel scale: perceptually relevant scale for pitch.</li>
        <li>\( m = 2595. \log(1 + \frac{f}{700} )\) </li>
        <li> \( f = 700. (10^{m/2595} - 1) \)</li>
        <li>1000 Hz = 1000 Mel</li>

        <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.5.0/Chart.min.js"></script>
        <canvas id="myChart" style="width:100%;max-width:500px"></canvas>
        <script>
            var xValues = [];
            var yValues = [];
            generateData("2595*Math.log10(1+ x/700)", 0, 2500, 200);
            

            new Chart("myChart", {
            type: "line",
            data: {
                labels: xValues,
                datasets: [{
                fill: false,
                pointRadius: 2,
                borderColor: "rgba(0,0,255,0.5)",
                data: yValues
                }]
            },    
            options: {
                legend: {display: false},
                title: {
                display: true,
                text: "Mel scale",
                fontSize: 16
                },

            scales: {
            yAxes: [{scaleLabel: {display: true,labelString: 'Mel frequency'} }],
            xAxes: [{scaleLabel: {display: true,labelString: 'Frequency (kHz)'} }],
                }     
            }
            });
            function generateData(value, i1, i2, step = 1) {
            for (let x = i1; x <= i2; x += step) {
                yValues.push(eval(value));
                xValues.push(x);
            }
            }
        </script>

    <li> Recipe to extract mel spectrogram:</li>
    <ul>
        <li>Extract STFT</li>
        <li>Convert amplitude to DBs</li>
        <li>Convert frequencies to Mel scale (steps below):</li>
        <ul>
            <li>Choose number of mel bands (how many: depends- hyperparameter, generally 40 - 130)</li>
            <li>Construct mel filter banks: (multi-step)</li>
            <li>Apply mel filter banks to spectrogram</li>
        </ul>
        <li>Steps to construct mel filter banks:</li>
        <ul>
            <li>Convert lowest/highest frequency to mel using below formula</li>
            <li>\( m = 2595 . log( 1 + \frac{f}{700}) \)</li>
            <li>Create number of mel bands equally spaced points. (example for 6 bands) </li>
            <li> Convert points back to hertz</li>
            <li>\( f = 700 (10^{m/2595} - 1 )\)</li>
            <li>Round to nearest frequency bin</li>
            <li>Create triangular filters</li>  

            <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
            <div id="myPlot" style="width:100%;max-width:200px"></div>

            <!-- <script>
                // TODO : complete the picture with animation of this process
                var y1 = [0,0,0,0,0,0];
                var x1 = [1,2,3,4,5,6];
                var y2 = [0,0];
                var x2 = [0,7];
                var firstPlotTrace = {
                    x: x2,
                    y: y2,
                    mode: 'lines+markers',
                    marker: {color: 'rgb(255, 0, 0)'},
                    line: {color: 'rgb(0, 255, 0)',width: 2},
                    name: 'end pts'
                };
                var secondPlotTrace = 
                    {
                        x: x1,
                        y: y1,
                        mode: 'markers',
                        marker: {color: 'rgb(0, 0, 255)'},
                        name: 'filter'
                    };

                var plotData = [firstPlotTrace, secondPlotTrace];

                var layout = {
                    height: 300,
                    width: 700,
                    yaxis: {range: [-0.1, 0.1],zeroline: false,showline: false, visible: false  },
                    xaxis: {range:[-1,8], zeroline: false,showline: false , visible: false }, 
                    displaylogo: false
                    //showlegend: false

                };
                Plotly.newPlot('myPlot', plotData, layout, {responsive:false});
            
            </script>      -->
        </ul>
    <li>Mel spectrogram = matrix multiplication between Melfilters and Spectrogram = M Y</li>
    <li>Mel spectrogram shape: (# bands, # frames)</li>
    <li>Mel spectrogram applications: Audio classification, Automatic mood recognition, music genre classification, music instrument classification </li>

    </ul>
    </ul>
    <h3>Mel-Frequency cepstral coefficients</h3>
    <ul>
        <li>cepsral : cepstrum: ceps: spec: spectrum</li>
        <li>ceptrum developed while studying echos in siesmic signals. (1960s)</li>
        <li>Audio feature of choice for speech recognition/identification (1970s)</li>
        <li>Music processing (2000s)</li>
        <li>quefrency: Frequency</li>
        <li>liftering: filtering</li>
        <li>Rhamonic: harmonic</li>
        <li>Computing the cepstrum</li>
        <li>\( C(x(t)) = F^{-1}\big[log(F[x(t)])\big] \) </li>

        <li>Speech generation:  glottal pulses \(\rightarrow\) Vocal Tract \(\rightarrow\) Speech Signal</li>
        <li> Glottal pulse contains info about pitch, vocal tract about the timbre (vowels and consonents)</li>
        <li>log spectrum \( \rightarrow\) get maximum spectral envelope \( \rightarrow \) the peaks which we get are called Formants \(\rightarrow \)these carry identity of sound</li>
        <li>Spectral envelope contains Vocal tract frequency response (Impulse response)</li>
        <li>Speech is convolution of vocal tract frequency response with glottal pulse.</li>
        <figure><img src="../images/cepstrum_1.png" height="400px">
            <img src="../images/speech_process.png" height="300px">
        </figure>
        <li>\( x(t) = e(t)\otimes h(t) \) </li>
        <li> X(f) = E(f). H(f)  </li>
        <li>log (X(f)) = log (E(f)) + log(H(f))</li>
        <li> We donot get these two as separate, the goal is to separate these two components.</li>
        <li>And we are not interested in the timbre, interested in formants.</li>
        <li>We want to remove the high frequencies in the log power spectrum: low pass filter : liftering</li>
        <li>Steps:</li>
        <ul>
            <li>Waveform</li>
            <li>DFT</li>
            <li>Log-Amplitude spectrum (we donot perceive amplitude linearly)</li>
            <li>Mel scaling (linear frequency rep to mel based)</li>
            <li>Discrete cosine transform (similar to inverse fourier transform)</li>
            <li>This gives MFCCs</li>
        </ul>
        <li>Why use Discrete cosine transform?</li>
        <li>DIscerete cosine transform is simplified version of fourier transform. We get real valued coefficients</li>
        <li>Decorrelates energy in different mel bands: (actual mel bands are overlapping)</li>
        <li>Reduce # dimensions to represent spectrum.</li>
        <li>How many coefficients?</li>
        <li>first 12-13 coeff</li>
        <li>First coefficients keep most of the information (eg. formants, spectral envolope)</li>
        <li>Use \( \Delta \) and \( \Delta \Delta\) MFCCs: taking first and second derivatives (subtract MFCC's from previous to get delta)</li>
        <li>Total 39 coefficients per frame (13x3, mfcc, \(\Delta, \Delta\Delta\) )</li>
        <li>Advantages:</li>
        <ul>
            <li>Describe the large structures of the spectrum</li>
        <li>Ignore fine spectral structures.</li>
        <li>Work well in speech and music processing.</li>
        </ul>
        <li>Disadvantages:</li>
        <ul>
            <li>Not robust to noise</li>
            <li>Extensive knowledge engineering</li>
            <li>Not efficient for synthesis</li>
        </ul>
        <li>Applications:</li>
        <ul>
            <li>Speech prcessing: speech recognition, speaker recognition</li>
            <li>Music processing: Music genre classification, mood classification, Automatic tagging</li>
            <li>Pitch invariant, Timbre centric, cannot use them for chord classification.</li>
        </ul>

    </ul>
<h3>Pytorch for audio applications</h3>
<h4>Torchaudio</h4>
<ul>
    <li>audio processing library for pytorch</li>
    <li>Audio datasets</li>
    <li>Data augmentation</li>
    <li>Feature extraction</li>
    <li>Everything will happen on GPU: so faster</li>
</ul>

<h3>Connectionist Temporal Classification (CTC).</h3>
<h4>Derivation:</h4>
<ul>
    <li>Label Error Rate (LER) is defined as:
        <p>LER(h, S') = \( \frac{1}{|S'|} \sum_{(x,z) \in S'} \frac{\text{ED}(h(x), z)}{|z|} \)</p>
        </li>        
    <li>Alignment problem (x,z), x is input sequence \( (x_1, x_2 ... x_T) \), z - target sequence  \(( z_1, z_2 ... z_U) \) U <=T</li>
    <li>Define RNN:   \((&Ropf;^m)^T &rarr;  (&Ropf;^n)^T \)</li>
    <li>The conditional probability of a label sequence is defined as:
        <p>\( p(\pi |x) = \prod_{t=1}^{T} y_{\pi_t}^t, \forall \pi \in L'^{T} \)</p>
        </li>
    <li>Then \( y_t^k \) is interpreted as the probability of observing label \( k \) at time \( t \), which defines a distribution over the set \( L'^T \) of length \( T \) sequences over the alphabet \( L' = L \cup \{\text{blank}\} \):</li>
    <li>From now on, we refer to the elements of \( L'^T \) as paths, and denote them \( \pi \).</li>
    <li>The next step is to define a many-to-one map \( B : L'^T \rightarrow L^{\leq T} \), where \( L^{\leq T} \) is the set of possible labellings (i.e. the set of sequences of length less than or equal to \( T \) over the original label alphabet \( L \)).</li>

    <li>We do this by simply removing all blanks and repeated labels from the paths (e.g. \( B(a − ab−) = B(−aa − −abb) = aab \)). Intuitively, this corresponds to outputting a new label when the network switches from predicting no label to predicting a label, or from predicting one label to another.</li>
    <p>Finally, we use B to define the conditional probability of a given labelling \(l \in L^{≤T}\) as the sum of the probabilities of all the paths corresponding to it.</p>
    <li>The probability of a given labelling is defined as:
        <p>p(l|x ) = \( \sum_{\pi \in B^{-1}(l)} p(\pi|x) \)</p>
        </li>
    <li>The forward variable \( \alpha \) is defined as:
        <p> \( \alpha_t(s) =\sum_{\pi \in N^T ;  B(\pi_{1:t})=l_{1:s} }\prod_{t'=1}^{t} y^{\pi_{t'}}_{t'} \)</p>
    </li>
    # udpate forward recursion
    <li> The forward variable is computed by summing over all paths that end in the label sequence up to time \( t \), and multiplying the probabilities of the labels along the path.</li>
    <li> The forward variable is computed recursively as follows:</li>
    <li> \( \alpha_1(1) = y^{\text{b}}_1, \alpha_1(2) = y^1_{l_1},  \alpha_1(s) = 0  \forall  s > 1 \)</li>


    
</ul>

<h3>Chi Square Distribution</h3>
<h4>Formulation:</h4>
<p>
   Likelihood  \( \mathcal{L}  = P( \mathbf{D}|\mathbf{\theta}, M) =\displaystyle \prod_{i=1}^N \dfrac{\exp\left(\dfrac{-r_i^2}{2\sigma_i^2}\right)}{\sqrt{2\pi}\sigma} \)
</p>
<p>
    \( \log \mathcal{L} = \frac{1}{2} \displaystyle \sum_{i=1}^N \bigg( -\log(2\pi) - \log(\sigma_i^2) - \frac{r_i^2}{\sigma_i^2}  \bigg) \)
</p>

<p>
    If \( \sigma_i \) is constant, then
</p>
<p>
    \( \log L = c - \frac{1}{2} \chi^2 \)
</p>
<p>
    It’s often more convenient to calculate \( \log \mathcal{L} = \mathcal{L}^2 \)
</p>

<p>
    If \( Z_1, \ldots, Z_k \) are independent, standard normal random variables, then the sum of their squares,
</p>
<p>
    \( Q = \displaystyle \sum_{i=1}^{k} Z_i^2, \)
</p>
<p>
    is distributed according to the chi-squared distribution with \( k \) degrees of freedom.
</p>

<h3>Markov Chain Monte Carlo</h3>
<h4>Metropolis algorithm:</h4>

<h3>Weighted Finite State transducer (WFST)</h3>
Example: Writing Regular expression ab*cd+e
<ul>
    <li>a: The string starts with the character a.</li>
    <li>b*: Zero or more occurrences of the character b.</li>
    <li>c: Followed by the character c.</li>
    <li>d+: One or more occurrences of the character d.</li>
    <li>e: Ends with the character e.</li>
</ul>

<figure>
<img src="../images/abcde.png" alt="wfst image">
</figure>
<h4>Code to visualize wfst in ascii format</h4>

<pre><code>
    import pynini
    
    teens = pynini.string_map([
        ("onze", "11"),
        ("douze", "12"),
        ("treize", "13"),
        ("quatorze", "14"),
        ("quinze", "15"),
        ("seize", "16"),
    ])
    
    teens.write("teens.fst")
        </code></pre>
    
        <h4>Command to Generate High-Resolution Image</h4>
        <pre><code>
    fstdraw --isymbols=ascii.syms -portrait teens.fst | dot -Tpng -Gdpi=300 > teens_high_res.png
        </code></pre>

<h4>arguments in python</h4>
<pre><code>
def example_function(pos1, pos2, *args, kw1, kw2="default", **kwargs):
    print(f"pos1: {pos1}, pos2: {pos2}")
    print(f"args: {args}")
    print(f"kw1: {kw1}, kw2: {kw2}")
    print(f"kwargs: {kwargs}")

# Calling the function
example_function(1, 2, 3, 4, 5, kw1="value1", extra1="extra", extra2="extra2")
output:
pos1: 1, pos2: 2
args: (3, 4, 5)
kw1: value1, kw2: default
kwargs: {'extra1': 'extra', 'extra2': 'extra2'}
</code></pre>
<h4>Decorator example</h4>
<pre><code>
def my_decorator(func):
    def wrapper():
        print("before the function call")
        func()
        print("after the function call")
    return wrapper

@my_decorator
def say_hello():
    print("Hello!")

say_hello()
</code></pre>
<h4>Sort a dictionary based on key or value </h4>
    <pre><code>
        sorted(d.items(), key=lambda x: x[1]) #value
        sorted(d.items(), key=lambda x: x[0]) #key
    </code></pre>

<h4>EM algorithm</h4>
Convert the probablity into a an expectation and then use Jenson's inquality. z is hidden variable.
<p>
    \( \log p(x; \theta) = \log \sum_z p(x, z; \theta) \)
  </p>
  <p>
    \( = \log \sum_z \frac{Q(z)}{Q(z)} p(x, z; \theta) \)
  </p>
  <p>
    \( = \log E_{z \sim Q} \left[ \frac{p(x, z; \theta)}{Q(z)} \right] \)
  </p>
  <p>
    \( \geq E_{z \sim Q} \left[ \log \frac{p(x, z; \theta)}{Q(z)} \right] \)
  </p>
  <p>
    \( = \text{ELBO}(x; Q, \theta) \)
  </p>
  <h5>Corollary</h5>
  <p>
    \( \log p(x; \theta) = \text{ELBO}(x; Q, \theta) \iff Q(z) = p(z | x; \theta) \)
  </p>
  <h5>Additional Equations</h5>
  <p>
    \( E_{z \sim Q} [g(z)] = \sum_z Q(z) g(z) \)
  </p>
  <p>
    \( g(z) = \frac{p(x, z; \theta)}{Q(z)} \)
  </p>

  <h5>EM Algorithm Steps</h5>
  <p>Randomly initialize \( \theta \)</p>
  <p><b>Loop till convergence</b></p>
  <ul>
    <li><b>E-Step</b>: For each \( i \)</li>
    <ul>
      <li>\( Q_i(z) := P(z^{(i)} | x^{(i)}; \theta) \)</li>
    </ul>
    <li><b>M-Step</b></li>
    <ul>
      <li>\( \theta := \arg \max_\theta \sum_{i=1}^n \text{ELBO}(x^{(i)}; \theta) \)</li>
      <li>\( = \arg \max_\theta \sum_{i=1}^n \sum_{z} Q_i(z) \log \frac{P(x, z; \theta)}{Q_i(z)} \)</li>
    </ul>
  </ul>

  <h4>Beta function equation:</h4>
  <p>
      \[
      \mathrm{B}(\alpha_{1},\alpha_{2},\ldots \alpha_{n}) = \frac{\Gamma(\alpha_{1}) \, \Gamma(\alpha_{2}) \cdots \Gamma(\alpha_{n})}{\Gamma(\alpha_{1} + \alpha_{2} + \cdots + \alpha_{n})}
      \]
  </p>
  <p>
    \[
    \mathrm{B}(z_{1}, z_{2}) = \int_{0}^{1} t^{z_{1}-1} (1-t)^{z_{2}-1} \, dt
    \]
  </p>
  <p>
    \[
        \mathrm{B}(z_{1}, z_{2}) = \frac{\Gamma(z_{1}) \, \Gamma(z_{2})}{\Gamma(z_{1} + z_{2})}
        \]
  </p>
  <p>
    \[
    f(x; a, b) = \frac{x^{a-1} (1-x)^{b-1}}{B(a,b)} = \frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} x^{a-1} (1-x)^{b-1}
    \]
</p>
<h4>Inspect a code (find location of a function from where it is called) </h4>
x=model.classify_batch(signal)
how to find the location of this function?
<pre><code>
    import inspect
    # Assuming `model.classify_batch` is a method
    print(inspect.getsourcefile(model.classify_batch))
</code></pre>
<h4>Custom functions in app script</h4>
<pre>
    <code>
        function onOpen() {
            var ui = SpreadsheetApp.getUi();
            ui.createMenu('Sumit Menu')
              .addItem('Insert the date', 'insertDate')
              .addToUi();
          }
          
          /**
           * Multiplies the input value by 2.
           *
           * @param {number} input The value to multiply.
           * @return The input multiplied by 2.
           * @customfunction
           */
          function DOUBLE(input) {
            return input * 2;
          }
          
          function insertDate() {
            var sheet = SpreadsheetApp.getActiveSpreadsheet().getActiveSheet();
            // var cell = sheet.getRange('B2');
            var cell = sheet.getActiveRange(); 
            cell.setValue(new Date());
          }
    </code>
</pre>
</body>
</html>